import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time
from flask import Flask, request, jsonify, send_file, render_template
from flask_cors import CORS
import os
import json
import time
import pandas as pd
from pathlib import Path
from werkzeug.utils import secure_filename
from openai import OpenAI
import re
import jieba
import jieba.analyse
from typing import Dict, List, Optional, Union
import subprocess
import tempfile
import shutil
import whisper
import sys
import traceback
import threading
from datetime import datetime

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# åŠ è½½ .env æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
try:
    from dotenv import load_dotenv
    # å°è¯•ä»å¤šä¸ªä½ç½®åŠ è½½ .env æ–‡ä»¶
    env_paths = [
        Path(__file__).parent / 'templates' / '.env',  # templates ç›®å½•ï¼ˆweb/Vedio-content-summary-website/templates/.envï¼‰
        Path(__file__).parent / '.env',  # å½“å‰ç›®å½•ï¼ˆweb/Vedio-content-summary-website/.envï¼‰
        Path(__file__).parent.parent.parent / '.env',  # é¡¹ç›®æ ¹ç›®å½•ï¼ˆå‰æ²¿è®ºå›è¯†åˆ«è§†é¢‘/.envï¼‰
    ]
    loaded = False
    for env_path in env_paths:
        if env_path.exists():
            load_dotenv(env_path)
            print(f"âœ… å·²åŠ è½½ .env æ–‡ä»¶: {env_path}")
            loaded = True
            break
    if not loaded:
        print(f"â„¹ï¸ æœªæ‰¾åˆ° .env æ–‡ä»¶ï¼Œå°è¯•ä½ç½®: {[str(p) for p in env_paths]}")
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä»å½“å‰å·¥ä½œç›®å½•åŠ è½½
        load_dotenv()  # è¿™ä¼šæŸ¥æ‰¾å½“å‰ç›®å½•å’Œçˆ¶ç›®å½•çš„ .env æ–‡ä»¶
except ImportError:
    print("âš ï¸ python-dotenv æœªå®‰è£…ï¼Œæ— æ³•åŠ è½½ .env æ–‡ä»¶ã€‚å¦‚éœ€ä½¿ç”¨ .envï¼Œè¯·è¿è¡Œ: pip install python-dotenv")

app = Flask(__name__)
CORS(app)

# ==========================================
# ğŸ‘‡ çˆ¬è™«è¾…åŠ©å‡½æ•° (ä»ä½ çš„ scraper.py ç§»æ¤)
# ==========================================

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/120.0.0.0 Safari/537.36"
}

def fetch_soup(url: str, timeout: int = 20) -> BeautifulSoup:
    """è·å–å¹¶è§£æç½‘é¡µï¼Œå¤„ç†ç¼–ç é—®é¢˜"""
    try:
        r = requests.get(url, headers=HEADERS, timeout=timeout)
        r.raise_for_status()
        html = r.content.decode("utf-8", errors="replace")
        return BeautifulSoup(html, "lxml") # å¦‚æœæŠ¥é”™ lxml not foundï¼Œè¯·æ”¹æˆ "html.parser"
    except Exception as e:
        print(f"Error fetching {url}: {e}")
        return None

def parse_year_links(root_url: str) -> list[dict]:
    """è§£æå¹´ä»½åˆ—è¡¨"""
    soup = fetch_soup(root_url)
    if not soup: return []

    years = []
    seen = set()
    for a in soup.select("div.luntan-year-swiper a.year-box"):
        year_span = a.select_one("span.big")
        if not year_span: continue
        year = year_span.get_text(strip=True)
        href = a.get("href", "").strip()
        if not href: continue
        year_url = urljoin(root_url, href)
        if year in seen: continue
        seen.add(year)
        years.append({"year": year, "year_url": year_url if year_url.endswith("/") else year_url + "/"})
    return years

def parse_year_forums(year_url: str) -> list[dict]:
    """è§£ææŸå¹´ä»½ä¸‹çš„æ‰€æœ‰è®ºå›ç‰ˆå—"""
    soup = fetch_soup(year_url)
    if not soup: return []
    forums = []
    seen = set()
    # é€»è¾‘å’Œä½ ä¹‹å‰çš„ä¸€æ ·
    for a in soup.select('a[href*="/d"][href$="/"]'):
        href = a.get("href", "").strip()
        if not href: continue
        forum_url = href if href.startswith("http") else urljoin(year_url, href)
        if "/xshd/kxyjsqylt/" not in forum_url: continue
        if f"/{year_url.rstrip('/').split('/')[-1]}/" not in forum_url: continue
        forum_id = forum_url.rstrip("/").split("/")[-1]
        if forum_id in seen: continue
        seen.add(forum_id)
        forums.append({"forum_id": forum_id, "forum_url": forum_url})
    return forums

def parse_page_urls(forum_url: str) -> list[str]:
    """è§£æè¯¥è®ºå›ä¸‹çš„æ‰€æœ‰æ–‡ç« URL (detail_url)"""
    soup = fetch_soup(forum_url) # é»˜è®¤åªçˆ¬ç¬¬ä¸€é¡µä»¥èŠ‚çœæ—¶é—´ï¼Œå¦‚éœ€å…¨çˆ¬å‚è€ƒ scraper.py
    if not soup: return []
    
    urls = []
    # ä½¿ç”¨ä½ ä¿®æ”¹åçš„ parse_page é€»è¾‘ï¼Œè¿™é‡Œåªæå– url
    for li in soup.select("ul#content > li.tuwen-list"):
        twen = li.select_one("div.twen-info")
        if not twen: continue
        
        # è·å–è¯¦æƒ…é¡µé“¾æ¥
        title_a = twen.select_one("a.overfloat-dot-2")
        if title_a and title_a.has_attr("href"):
            detail_url = urljoin(forum_url, title_a["href"])
            if detail_url:
                urls.append(detail_url)
    return urls


GEMINI_MODEL = "gemini-2.5-pro"
OPENAI_MODEL = "gpt-5.1"
DEEPSEEK_MODEL = "deepseek-chat"
QWEN_MODEL = "qwen-max"
QWEN_BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"

UPLOAD_FOLDER = Path("uploads")
TRANSCRIPTS_FOLDER = Path("transcripts")
TRANSCRIPTS_CLEAN_FOLDER = Path("transcripts_clean")
UPLOAD_FOLDER.mkdir(exist_ok=True)
TRANSCRIPTS_FOLDER.mkdir(exist_ok=True)
TRANSCRIPTS_CLEAN_FOLDER.mkdir(exist_ok=True)

ALLOWED_EXTENSIONS = {'mp4', 'avi', 'mov', 'mkv', 'flv', 'webm', 'mp3', 'wav', 'm4a'}

MAX_KEYWORDS_TOTAL = 30
MIN_WORD_LENGTH = 2
MAX_TRANSCRIPT_LENGTH = 3000
MAX_DESCRIPTION_LENGTH = 500

COMMON_WORDS = [
    'ç ”ç©¶', 'æ–¹æ³•', 'æŠ€æœ¯', 'ç³»ç»Ÿ', 'é—®é¢˜', 'åº”ç”¨', 'å‘å±•', 'å®ç°',
    'åˆ†æ', 'è®¾è®¡', 'å¼€å‘', 'æ„å»º', 'ä¼˜åŒ–', 'æå‡º', 'åˆ›æ–°', 'æ”¹è¿›',
    'éªŒè¯', 'æµ‹è¯•', 'è¯„ä¼°', 'ç­–ç•¥', 'æ–¹æ¡ˆ', 'å·¥è‰º', 'è¿‡ç¨‹', 'ç»“æœ',
    'æ•°æ®', 'ä¿¡æ¯', 'å†…å®¹', 'æ–¹é¢', 'é¢†åŸŸ', 'æ–¹å‘', 'å·¥ä½œ', 'é¡¹ç›®'
]

METHOD_KEYWORDS = [
    'è®¾è®¡', 'å¼€å‘', 'æ„å»º', 'ä¼˜åŒ–', 'å®ç°', 'æå‡º', 'åˆ›æ–°',
    'æ”¹è¿›', 'ç ”ç©¶', 'åˆ†æ', 'éªŒè¯', 'æµ‹è¯•', 'è¯„ä¼°', 'åº”ç”¨',
    'æ–¹æ³•', 'ç­–ç•¥', 'æ–¹æ¡ˆ', 'æŠ€æœ¯', 'å·¥è‰º'
]

FILLER_WORDS_CHINESE = [
    'è¿™ä¸ª', 'é‚£ä¸ª', 'ç„¶å', 'å°±æ˜¯', 'å°±æ˜¯è¯´',
    'å—¯', 'å‘ƒ', 'å•Š', 'é‚£ä¸ªé‚£ä¸ª', 'è¿™ä¸ªè¿™ä¸ª', 'ç„¶åç„¶å',
    'å‘ƒå‘ƒ', 'å—¯å—¯', 'å•Šå•Š'
]

FILLER_WORDS_ENGLISH = [
    r'\bum\b', r'\buh\b', r'\byou know\b', r'\blike\b'
]

GEMINI_AVAILABLE = False
FEW_SHOT_EXAMPLE_TEXT = ""

try:
    from google import genai
    from google.genai import types
    import trafilatura
    from yt_dlp import YoutubeDL
    GEMINI_AVAILABLE = True
    
    try:
        gemini_try_path = Path(__file__).parent.parent.parent / "gemini_try.py"
        if gemini_try_path.exists():
            with open(gemini_try_path, 'r', encoding='utf-8') as f:
                content = f.read()
                match = re.search(r'FEW_SHOT_EXAMPLE_TEXT\s*=\s*"""(.*?)"""', content, re.DOTALL)
                if match:
                    FEW_SHOT_EXAMPLE_TEXT = match.group(1).strip()
    except:
        pass
except ImportError as e:
    print(f"è­¦å‘Š: æ— æ³•å¯¼å…¥Geminiç›¸å…³æ¨¡å—: {e}")

try:
    from openai import OpenAI as DeepSeekClient
    DEEPSEEK_AVAILABLE = True
except:
    DEEPSEEK_AVAILABLE = False

try:
    jieba.initialize()
except:
    pass

# RAGç›¸å…³é…ç½®å’Œåˆå§‹åŒ–
RAG_AVAILABLE = False
RAG_ENGINE = None
RAG_ENGINE_API_KEY = None  # è·Ÿè¸ªå½“å‰RAGå¼•æ“ä½¿ç”¨çš„API Key

try:
    from llama_index.core import VectorStoreIndex
    from llama_index.core.vector_stores import MetadataFilter, MetadataFilters, FilterOperator
    from llama_index.embeddings.google_genai import GoogleGenAIEmbedding
    from llama_index.llms.google_genai import GoogleGenAI
    from llama_index.core import Settings
    try:
        from llama_index.vector_stores.postgres import PGVectorStore
    except ImportError:
        try:
            from llama_index_postgres import PGVectorStore
        except ImportError:
            PGVectorStore = None
    
    RAG_AVAILABLE = True
except ImportError as e:
    print(f"è­¦å‘Š: RAGæ¨¡å—æœªå®‰è£…: {e}")

# RAGæ•°æ®åº“é…ç½®
PROJECT_REF = "ivngzzdpdvcryhzevhsi"
RAG_DB_CONFIG = {
    'DB_PASSWORD': "ZssHjq19880302_",
    'PROJECT_REF': PROJECT_REF,
    'DB_NAME': "postgres",
    'DB_USER': "postgres",
    'DB_HOST': f"db.{PROJECT_REF}.supabase.co",
    'DB_PORT': "5432",
    'TABLE_NAME': "cas_reports"
}

# ç®¡ç†å‘˜å¯†ç ï¼ˆç”¨äºåå°ç®¡ç†é¡µé¢ï¼‰
ADMIN_PASSWORD = os.environ.get('ADMIN_PASSWORD', 'admin123')  # é»˜è®¤å¯†ç ï¼Œå»ºè®®é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®

BROAD_CATEGORIES = [
    "æ•°å­¦", "åŠ›å­¦", "ç‰©ç†å­¦", "åŒ–å­¦", "å¤©æ–‡å­¦", "åœ°çƒç§‘å­¦", "ç”Ÿç‰©å­¦",
    "å†œå­¦", "æ—å­¦", "ç•œç‰§/å…½åŒ»ç§‘å­¦", "æ°´äº§å­¦", "åŒ»è¯å«ç”Ÿ",
    "æµ‹ç»˜ç§‘å­¦æŠ€æœ¯", "ææ–™ç§‘å­¦ä¸å·¥ç¨‹", "å†¶é‡‘å·¥ç¨‹æŠ€æœ¯", "æœºæ¢°ä¸è¿è½½å·¥ç¨‹",
    "æ ¸ç§‘å­¦æŠ€æœ¯", "ç”µå­ä¸é€šä¿¡æŠ€æœ¯", "è®¡ç®—æœºç§‘å­¦æŠ€æœ¯", "åŒ–å­¦å·¥ç¨‹",
    "è½»å·¥çººç»‡ç§‘å­¦æŠ€æœ¯", "é£Ÿå“ç§‘å­¦æŠ€æœ¯", "åœŸæœ¨å»ºç­‘å·¥ç¨‹", "æ°´åˆ©å·¥ç¨‹",
    "äº¤é€šè¿è¾“", "èˆªç©ºèˆªå¤©ç§‘å­¦æŠ€æœ¯", "ç¯å¢ƒåŠèµ„æº", "å®‰å…¨ç§‘å­¦æŠ€æœ¯",
    "ç®¡ç†å­¦", "ç¤¾ä¼šäººæ–‡", "å…¶ä»–"
]

def create_openai_client(api_key: str, base_url: str = None):
    old_key = os.environ.get('OPENAI_API_KEY', None)
    try:
        os.environ['OPENAI_API_KEY'] = api_key
        if base_url:
            client = OpenAI(api_key=api_key, base_url=base_url)
        else:
            client = OpenAI()
        return client
    except Exception as e:
        try:
            if 'OPENAI_API_KEY' in os.environ:
                del os.environ['OPENAI_API_KEY']
            client = OpenAI(api_key=api_key, base_url=base_url) if base_url else OpenAI(api_key=api_key)
            return client
        except Exception as e2:
            if old_key: os.environ['OPENAI_API_KEY'] = old_key
            raise Exception(f"OpenAI å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)} | {str(e2)}")
    finally:
        if old_key and 'OPENAI_API_KEY' not in os.environ:
            os.environ['OPENAI_API_KEY'] = old_key

def create_qwen_client(api_key: str):
    """åˆ›å»ºQwenï¼ˆé˜¿é‡Œäº‘ç™¾ç‚¼ï¼‰å®¢æˆ·ç«¯ï¼Œä½¿ç”¨OpenAIå…¼å®¹æ¥å£"""
    return create_openai_client(api_key, base_url=QWEN_BASE_URL)

def create_gemini_client(api_key: str):
    if not GEMINI_AVAILABLE:
        raise ImportError("Gemini æ¨¡å—æœªå®‰è£…æˆ–ä¸å¯ç”¨")
    from google import genai
    return genai.Client(api_key=api_key)

def cleanup_files(*file_paths):
    for path in file_paths:
        if path and os.path.exists(path):
            try:
                os.remove(path)
            except Exception as e:
                print(f"æ¸…ç†æ–‡ä»¶å¤±è´¥ {path}: {e}")

def transcribe_with_whisper(filepath: str) -> str:
    """
    """
    print(f"å¼€å§‹ Whisper è½¬å½•: {filepath}")
    
    try:
        model_name = "small"
        model = None
        
        try:
            model = whisper.load_model("small")
            print("å·²åŠ è½½ small æ¨¡å‹")
        except Exception:
            try:
                print("åŠ è½½ small å¤±è´¥ï¼Œå°è¯• base")
                model_name = "base"
                model = whisper.load_model("base")
            except Exception:
                print("åŠ è½½ base å¤±è´¥ï¼Œå°è¯• tiny")
                model_name = "tiny"
                model = whisper.load_model("tiny")
        
        result = model.transcribe(
            str(filepath),
            language="zh",
            task="transcribe",
            fp16=False,
            verbose=False,
            initial_prompt="""è¿™æ˜¯ä¸€æ®µå­¦æœ¯æ¼”è®²ï¼ŒåŒ…å«ä¸“ä¸šæœ¯è¯­å’ŒæŠ€æœ¯å†…å®¹ï¼Œè¯·å°½é‡å‡†ç¡®åœ°è½¬å†™ã€‚
1. èƒ½è¯†åˆ«è¯­éŸ³è½¬å†™ï¼ˆASRï¼‰æˆ– OCR æ–‡æœ¬ä¸­å‡ºç°çš„ä¸“ä¸šæœ¯è¯­é”™è¯¯ï¼›
2. èƒ½å°†é”™è¯¯æˆ–ä¸è§„èŒƒçš„è¯è¯­ï¼Œæ ¡æ­£ä¸ºè¯¥é¢†åŸŸé€šç”¨ã€è§„èŒƒçš„å­¦æœ¯æœ¯è¯­ï¼›
è¾“å‡ºæ ¼å¼ï¼šåªè¾“å‡ºè½¬å†™åçš„æ­£æ–‡ï¼Œä¸è¦æ·»åŠ æ ‡é¢˜ã€æ³¨é‡Šã€è§£é‡Š"""
        )
        
        if "segments" in result and result["segments"]:
            return "\n".join([s.get("text", "").strip() for s in result["segments"] if s.get("text")])
        return result.get("text", "")

    except ImportError:
        print(f"Whisper Python API ä¸å¯ç”¨ï¼Œå°è¯•å‘½ä»¤è¡Œ: {filepath}")
        cmd = [
            'whisper', str(filepath),
            '--language', 'zh',
            '--output_dir', str(TRANSCRIPTS_FOLDER),
            '--output_format', 'txt',
            '--model', 'small',
            '--verbose', 'False',
            '--fp16', 'False'
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=1200)
        if result.returncode != 0:
            print("CLI small æ¨¡å‹å¤±è´¥ï¼Œå°è¯• base")
            cmd[6] = 'base'
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)
            
        if result.returncode != 0:
            raise Exception(f"Whisper å‘½ä»¤è¡Œå¤±è´¥: {result.stderr[:200]}")
            
        expected_output = TRANSCRIPTS_FOLDER / f"{Path(filepath).stem}.txt"
        if expected_output.exists():
            return expected_output.read_text(encoding='utf-8', errors='ignore')
        else:
            raise Exception("è½¬å½•æ–‡ä»¶æœªç”Ÿæˆ")

def generate_keyword_extraction_prompt(description: str, transcript: str) -> str:
    desc_short = description[:MAX_DESCRIPTION_LENGTH] if description else ""
    transcript_short = transcript[:MAX_TRANSCRIPT_LENGTH] if transcript else ""
    
    return f"""ä»ä»¥ä¸‹å†…å®¹æå–10-15ä¸ªå­¦æœ¯æœ¯è¯­å…³é”®è¯ï¼š

A) descriptionï¼ˆæ‘˜è¦ï¼Œæ ¸å¿ƒä¿¡æ¯ï¼‰:
{desc_short}

B) transcriptï¼ˆè½¬å†™æ–‡æœ¬ï¼Œè¯¦ç»†å†…å®¹ï¼‰:
{transcript_short}

æå–è¦æ±‚ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š
1. ã€å­¦æœ¯æœ¯è¯­ä¼˜å…ˆã€‘ä¼˜å…ˆæå–ä¸“ä¸šæœ¯è¯­ã€æŠ€æœ¯åè¯ã€å­¦ç§‘æ¦‚å¿µè¯
   - é•¿è¯ï¼ˆâ‰¥3å­—ï¼‰ä¼˜å…ˆï¼šå¦‚"ç¥ç»ç½‘ç»œ"ã€"ç”µåŒ–å­¦"ã€"é’™é’›çŸ¿"ã€"ç¦»å­ä¼ è¾“"
   - ä¸“ä¸šåè¯ä¼˜å…ˆï¼šå¦‚"æ™¶ä½“ç®¡"ã€"å™¨ä»¶"ã€"ç”µæ± "ã€"ç®—æ³•"
   - é¿å…é€šç”¨è¯ï¼šå¦‚"ç ”ç©¶"ã€"æ–¹æ³•"ã€"ç³»ç»Ÿ"ã€"åŠŸèƒ½"ã€"æ€§èƒ½"ã€"å®ç°"ã€"åº”ç”¨"ã€"å‘å±•"ã€"é—®é¢˜"ã€"æŠ€æœ¯"ã€"ç•Œé¢"ã€"ç­–ç•¥"ã€"æ–¹æ¡ˆ"

2. ã€æ¥æºçº¦æŸã€‘å…³é”®è¯å¿…é¡»åœ¨Aæˆ–Bä¸­æ˜ç¡®å‡ºç°ï¼ˆå…è®¸åŒä¹‰è¯/å˜ä½“ï¼‰

3. ã€è¿‡æ»¤è§„åˆ™ã€‘å¿…é¡»æ’é™¤ï¼š
   - æ³›è¯ï¼šç ”ç©¶/æ–¹æ³•/æŠ€æœ¯/ç³»ç»Ÿ/åŠŸèƒ½/æ€§èƒ½/å®ç°/åº”ç”¨/å‘å±•/é—®é¢˜/ç•Œé¢/ç­–ç•¥/æ–¹æ¡ˆ/è¿‡ç¨‹/ç»“æœ/æ•°æ®/ä¿¡æ¯/å†…å®¹/æ–¹é¢/é¢†åŸŸ/æ–¹å‘/å·¥ä½œ/é¡¹ç›®
   - æœºæ„åï¼šå­¦æ ¡åã€å…¬å¸å
   - äººå/åœ°å
   - ä½ä¿¡æ¯é‡è¯ï¼šå¦‚"ç•Œé¢"ã€"çŸ¥è¯†"ã€"åº”ç”¨"ã€"ç­–ç•¥"ã€"æ–¹æ¡ˆ"ã€"è¿‡ç¨‹"ã€"ç»“æœ"ã€"æ•°æ®"ã€"ä¿¡æ¯"ã€"å†…å®¹"ã€"æ–¹é¢"ã€"é¢†åŸŸ"ã€"æ–¹å‘"ã€"å·¥ä½œ"ã€"é¡¹ç›®"
   - å£è¯­åŒ–è¯æ±‡ï¼šå¦‚"åŸºæœ¬ä¸Š"ã€"æœ‰æ²¡æœ‰"ã€"ç›¸å½“äº"ã€"èˆ’æœ"ã€"è‚¯å®š"ã€"è¯å‡ºæ¥"ç­‰
   - åŠ¨è¯æ€§é€šç”¨è¯ï¼šå¦‚"èƒ½å¤Ÿ"ã€"æ”¶è·"ã€"æ¥æ°´"ç­‰

4. ã€è¯æ€§åå¥½ã€‘ä¼˜å…ˆåè¯æ€§è¯æ±‡ï¼ˆä¸“ä¸šæœ¯è¯­é€šå¸¸æ˜¯åè¯ï¼‰ï¼Œå‡å°‘åŠ¨è¯/å½¢å®¹è¯/å‰¯è¯

5. ã€è´¨é‡è¦æ±‚ã€‘
   - ç¡®ä¿æ¯ä¸ªå…³é”®è¯éƒ½æ˜¯è¯¥é¢†åŸŸçš„æ ¸å¿ƒå­¦æœ¯æœ¯è¯­
   - é¿å…æå–å£è¯­åŒ–è¡¨è¾¾ã€å¡«å……è¯ã€è¯­æ°”è¯
   - å…³é”®è¯åº”è¯¥å…·æœ‰æ˜ç¡®çš„å­¦æœ¯å«ä¹‰å’ŒæŠ€æœ¯ä»·å€¼

6. ã€æ•°é‡ã€‘æœ€ç»ˆè¾“å‡º10-15ä¸ªå…³é”®è¯ï¼Œç¡®ä¿éƒ½æ˜¯è¯¥é¢†åŸŸçš„æ ¸å¿ƒå­¦æœ¯æœ¯è¯­

è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š
{{
  "keywords": ["è¯1","è¯2",...]
}}"""

def generate_expert_opinion_prompt(web_text, transcript, few_shot_text, has_video=False):
    if has_video:
        input_section = f"""
    1. **ç½‘é¡µç®€ä»‹**ï¼š{web_text}
    2. **è§†é¢‘å†…å®¹**ï¼šè¯·é‡ç‚¹åˆ†æè§†é¢‘ä¸­çš„ PPT è§†è§‰ä¿¡æ¯ï¼ˆæ•°æ®/æ¶æ„å›¾ï¼‰å’Œ è¯­éŸ³è®ºè¿°ã€‚
"""
        visual_note = "å¦‚æœè§†é¢‘ PPT ä¸­å±•ç¤ºäº†å…³é”®æ•°æ®æˆ–æ¨¡å‹ï¼Œè¯·åœ¨æ–‡ç« ä¸­ç”¨æ–‡å­—æè¿°å‡ºæ¥ã€‚"
        visual_evidence_desc = "åˆ—å‡ºè§†é¢‘ä¸­3ä¸ªå…³é”®å›¾è¡¨/PPTé¡µé¢çš„è¯¦ç»†å†…å®¹"
    else:
        input_section = f"""
    1. **ç½‘é¡µç®€ä»‹**ï¼š{web_text if web_text else 'ï¼ˆæœªèƒ½æå–åˆ°ç½‘é¡µæ–‡å­—ï¼‰'}
    2. **è§†é¢‘è½¬å½•å†…å®¹**ï¼š{transcript[:5000] if transcript else 'ï¼ˆæ— è½¬å½•å†…å®¹ï¼‰'}
"""
        visual_note = "å¦‚æœè½¬å½•æ–‡æœ¬ä¸­æåˆ°äº†å…³é”®æ•°æ®æˆ–æ¨¡å‹ï¼Œè¯·åœ¨æ–‡ç« ä¸­ç”¨æ–‡å­—æè¿°å‡ºæ¥ã€‚"
        visual_evidence_desc = "åˆ—å‡ºè§†é¢‘ä¸­3ä¸ªå…³é”®å›¾è¡¨/PPTé¡µé¢çš„è¯¦ç»†å†…å®¹ï¼ˆå¦‚æœè½¬å½•æ–‡æœ¬ä¸­æœ‰æåˆ°ï¼‰"
    
    return f"""
    ä½ æ˜¯ç”±ä¸­å›½ç§‘å­¦é™¢å­¦éƒ¨åˆ†é…çš„èµ„æ·±å­¦æœ¯ç¼–è¾‘ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„å­¦æœ¯æŠ¥å‘Šè§†é¢‘ï¼Œæ’°å†™ä¸€ç¯‡é«˜è´¨é‡çš„ã€Šä¸“å®¶è§‚ç‚¹ã€‹æ–‡ç« ã€‚

    ### æ ¸å¿ƒå‚ç…§èŒƒä¾‹ (Style Reference)ï¼š
    è¯·æ¨¡ä»¿ä»¥ä¸‹èŒƒä¾‹çš„**å™è¿°å£å»**å’Œ**æ–‡ç« ç»“æ„**ï¼š
    {few_shot_text}
    
    ### è¾“å…¥èƒŒæ™¯èµ„æ–™ï¼š
{input_section}
    ### å†™ä½œä»»åŠ¡è¦æ±‚ï¼š
    1. **æ–‡ä½“é£æ ¼**ï¼šæ·±åº¦æŠ¥é“é£æ ¼ï¼Œä½¿ç”¨"æŠ¥å‘ŠäººæŒ‡å‡º"ç­‰ç¬¬ä¸‰äººç§°è¡¨è¿°ã€‚è¯­è¨€ä¸¥è°¨ã€å…¸é›…ã€‚
    2. **ç»“æ„è¦æ±‚**ï¼šçº¦ 1200 å­—ï¼Œå¿…é¡»åŒ…å«å°æ ‡é¢˜ï¼Œå¼€å¤´æ³¨æ˜è®ºå›åç§°ã€‚
    3. **è§†è§‰èåˆ**ï¼š{visual_note}
    
    ### è¾“å‡ºæ ¼å¼ (JSON)ï¼š
    {{
        "visual_evidence": {{
            "description": "{visual_evidence_desc}",
            "details": [{{"timestamp": "MM:SS", "content": "..."}}]
        }},
        "speaker_emphasis": {{
            "description": "...",
            "points": ["..."]
        }},
        "comprehensive_summary": "..."
    }}
    """

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def normalize_text(text: str) -> str:
    text = text.replace("\r\n", "\n").replace("\r", "\n")
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()

def remove_filler_words(text: str) -> str:
    for word in FILLER_WORDS_CHINESE:
        text = text.replace(word, '')
    for pattern in FILLER_WORDS_ENGLISH:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    return text

def remove_repeats(text: str) -> str:
    return re.sub(r'(\S+)(\s+|\W+)\1{2,}', r'\1', text)

def fix_punctuation(text: str) -> str:
    for char in ['ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ']:
        text = re.sub(f'[{char}]{{2,}}', char, text)
    return text

def merge_broken_sentences(text: str) -> str:
    lines = text.split('\n')
    merged = []
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        if len(line) < 10 and i + 1 < len(lines):
            next_line = lines[i + 1].strip()
            if next_line:
                merged.append(line + next_line)
                i += 2
                continue
        merged.append(line)
        i += 1
    return '\n'.join(merged)

def clean_text(text: str) -> str:
    text = normalize_text(text)
    text = remove_filler_words(text)
    text = remove_repeats(text)
    text = fix_punctuation(text)
    text = merge_broken_sentences(text)
    return normalize_text(text)

def extract_keywords_by_category(text: str) -> Dict[str, List[str]]:
    if not text or len(text.strip()) < 50:
        return {'all': [], 'methodology': []}
    
    keywords_noun = jieba.analyse.extract_tags(
        text, topK=MAX_KEYWORDS_TOTAL * 3, withWeight=True, allowPOS=('n', 'nz', 'nt', 'ns', 'nr')
    )
    keywords_verb_adj = jieba.analyse.extract_tags(
        text, topK=MAX_KEYWORDS_TOTAL * 2, withWeight=True, allowPOS=('v', 'vn', 'a', 'an')
    )
    
    all_candidates = {}
    for word, weight in keywords_noun + keywords_verb_adj:
        if len(word) >= MIN_WORD_LENGTH:
            if word not in all_candidates or weight > all_candidates[word]:
                all_candidates[word] = weight
    
    filtered_keywords = []
    sorted_words = sorted(all_candidates.items(), key=lambda x: x[1], reverse=True)
    long_words = [w for w, _ in sorted_words if len(w) >= 3]
    short_words = [w for w, _ in sorted_words if len(w) == 2]
    
    filtered_keywords.extend(long_words[:MAX_KEYWORDS_TOTAL])
    
    if len(filtered_keywords) < MAX_KEYWORDS_TOTAL:
        for word in short_words:
            if word not in filtered_keywords and word not in COMMON_WORDS:
                filtered_keywords.append(word)
                if len(filtered_keywords) >= MAX_KEYWORDS_TOTAL: break
                
    methodology_keywords = [w for w in filtered_keywords if any(mk in w for mk in METHOD_KEYWORDS)]
    
    return {
        'all': filtered_keywords[:MAX_KEYWORDS_TOTAL],
        'methodology': methodology_keywords
    }

def init_rag_engine(api_key=None):
    """åˆå§‹åŒ–RAGæœç´¢å¼•æ“ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
    global RAG_ENGINE, RAG_ENGINE_API_KEY
    
    # å¦‚æœAPI Keyæœªæä¾›ï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡è·å–
    if not api_key:
        import os
        api_key = os.environ.get('GOOGLE_API_KEY') or os.environ.get('GEMINI_API_KEY')
    
    if not api_key:
        raise ValueError("éœ€è¦æä¾›Gemini API Keyæ¥åˆå§‹åŒ–RAGå¼•æ“")
    
    # å¦‚æœå¼•æ“å·²å­˜åœ¨ä¸”API Keyç›¸åŒï¼Œç›´æ¥è¿”å›
    if RAG_ENGINE is not None and RAG_ENGINE_API_KEY == api_key:
        return RAG_ENGINE
    
    if not RAG_AVAILABLE:
        raise ImportError("RAGæ¨¡å—æœªå®‰è£…ï¼Œè¯·å®‰è£…: pip install llama-index llama-index-vector-stores-postgres")
    
    if not GEMINI_AVAILABLE:
        raise ImportError("Geminiæ¨¡å—æœªå®‰è£…ï¼Œè¯·å®‰è£…: pip install google-genai")
    
    try:
        # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆLlamaIndexçš„GoogleGenAIä¼šä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        import os
        os.environ["GOOGLE_API_KEY"] = api_key
        
        # é…ç½®LlamaIndexï¼ˆä¼šä»ç¯å¢ƒå˜é‡è¯»å–API Keyï¼‰
        Settings.embed_model = GoogleGenAIEmbedding(model_name="models/text-embedding-004")
        Settings.llm = GoogleGenAI(model="models/gemini-2.5-pro", temperature=0.1)
        
        # è¿æ¥PostgreSQLå‘é‡æ•°æ®åº“
        vector_store = PGVectorStore.from_params(
            database=RAG_DB_CONFIG['DB_NAME'],
            host=RAG_DB_CONFIG['DB_HOST'],
            password=RAG_DB_CONFIG['DB_PASSWORD'],
            port=RAG_DB_CONFIG['DB_PORT'],
            user=RAG_DB_CONFIG['DB_USER'],
            table_name=RAG_DB_CONFIG['TABLE_NAME'],
            embed_dim=768
        )
        
        index = VectorStoreIndex.from_vector_store(vector_store=vector_store)
        RAG_ENGINE = SmartSearchEngine(index)
        RAG_ENGINE_API_KEY = api_key  # ä¿å­˜API Keyä»¥ä¾¿åç»­æ£€æŸ¥
        
        return RAG_ENGINE
    except Exception as e:
        raise RuntimeError(f"RAGå¼•æ“åˆå§‹åŒ–å¤±è´¥: {str(e)}")

class SmartSearchEngine:
    """æ™ºèƒ½æœç´¢å¼•æ“ç±»ï¼ˆä»RAG_LlamdaIndex3.pyè¿ç§»ï¼‰"""
    def __init__(self, index):
        self.index = index
        self.analyzer_model = GEMINI_MODEL
    
    def analyze_query(self, user_query, api_key):
        """åˆ†æç”¨æˆ·æŸ¥è¯¢æ„å›¾"""
        if not GEMINI_AVAILABLE:
            return {"categories": [], "keywords": []}
        
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯æœç´¢åŠ©æ‰‹ã€‚ç”¨æˆ·æ­£åœ¨æœç´¢å­¦æœ¯æŠ¥å‘Šã€‚
        è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œæ¨æ–­å‡ºæœ€å¯èƒ½çš„ã€å®½æ³›å­¦ç§‘åˆ†ç±»ã€‘ï¼ˆ1-2ä¸ªï¼‰å’Œã€æ‰©å±•æ£€ç´¢è¯ã€‘ï¼ˆ3-5ä¸ªï¼‰ã€‚
        
        ã€å¯é€‰åˆ†ç±»åˆ—è¡¨ã€‘ï¼š{", ".join(BROAD_CATEGORIES)}
        ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š"{user_query}"
        
        è¦æ±‚ï¼š
        1. åˆ†ç±»å¿…é¡»å®Œå…¨æ¥è‡ªç»™å®šçš„åˆ—è¡¨ã€‚å¦‚æœæ— æ³•åˆ¤æ–­ï¼Œè¿”å›ç©ºåˆ—è¡¨ã€‚
        2. æ‰©å±•æ£€ç´¢è¯åº”è¯¥æ˜¯è¯¥é¢†åŸŸå…·ä½“çš„ä¸“ä¸šæœ¯è¯­ï¼Œç”¨äºå¸®åŠ©å‘é‡æ£€ç´¢æ‰¾åˆ°ç›¸å…³æ€§é«˜çš„å†…å®¹ã€‚
        3. è¯·ç›´æ¥è¾“å‡º JSON æ ¼å¼ï¼ŒåŒ…å« "categories" å’Œ "keywords" ä¸¤ä¸ªå­—æ®µã€‚
        
        ç¤ºä¾‹ JSON è¾“å‡ºï¼š
        {{
            "categories": ["åŒ–å­¦", "ç”Ÿç‰©å­¦"],
            "keywords": ["åˆ†å­åŠ¨åŠ›å­¦", "è›‹ç™½è´¨ç»“æ„", "è¯ç‰©é…ä½“", "æœ‰æœºåˆæˆ"]
        }}
        """
        
        try:
            client = create_gemini_client(api_key)
            response = client.models.generate_content(
                model=self.analyzer_model,
                contents=[types.Content(parts=[types.Part(text=prompt)])],
                config=types.GenerateContentConfig(
                    response_mime_type="application/json",
                    temperature=0.2
                )
            )
            text = response.text.replace("```json", "").replace("```", "").strip()
            analysis_result = json.loads(text)
            return analysis_result
        except Exception as e:
            print(f" æ„å›¾åˆ†æå¤±è´¥: {e}")
            traceback.print_exc()
            return {"categories": [], "keywords": []}
    
    def search(self, user_query, api_key, categories_filter=None):
        """æ‰§è¡Œæœç´¢"""
        analysis = self.analyze_query(user_query, api_key)
        target_categories = categories_filter if categories_filter else analysis.get("categories", [])
        expanded_keywords = analysis.get("keywords", [])
        
        filters = None
        if target_categories:
            filter_list = [
                MetadataFilter(key="categories", value=cat, operator=FilterOperator.CONTAINS)
                for cat in target_categories
            ]
            filters = MetadataFilters(filters=filter_list, condition="or")
        
        enhanced_query = f"{user_query} {' '.join(expanded_keywords)}"
        
        try:
            query_engine = self.index.as_query_engine(
                filters=filters,
                similarity_top_k=10
            )
            
            response = query_engine.query(enhanced_query)
            
            results_dict = {}  # key: titleæˆ–original_url, value: result with highest score
            if hasattr(response, 'source_nodes'):
                for node in response.source_nodes:
                    metadata = node.metadata if hasattr(node, 'metadata') else {}
                    text = node.text if hasattr(node, 'text') else ''
                    score = node.score if hasattr(node, 'score') else 0.0
                    
                    # ä½¿ç”¨titleæˆ–original_urlä½œä¸ºå”¯ä¸€æ ‡è¯†
                    unique_key = metadata.get('title') or metadata.get('original_url') or f"result_{len(results_dict)}"
                    
                    # æå–æ ¸å¿ƒæ‘˜è¦ï¼ˆä¼˜å…ˆä»metadataï¼Œå¦åˆ™ä»textä¸­æå–ï¼‰
                    summary = metadata.get('summary', '')
                    if not summary and text:
                        # å°è¯•ä»textä¸­æå–ã€æ ¸å¿ƒæ‘˜è¦ã€‘éƒ¨åˆ†
                        summary_match = re.search(r'ã€æ ¸å¿ƒæ‘˜è¦ã€‘\s*\n(.*?)(?=\n---|\n###|$)', text, re.DOTALL)
                        if summary_match:
                            summary = summary_match.group(1).strip()
                        else:
                            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ã€æ ¸å¿ƒæ‘˜è¦ã€‘ï¼Œä½¿ç”¨textçš„å‰500å­—ç¬¦ä½œä¸ºæ‘˜è¦
                            summary = text[:500] + ('...' if len(text) > 500 else '')
                    
                    # å¦‚æœè¯¥æŠ¥å‘Šå·²å­˜åœ¨ï¼Œæ¯”è¾ƒç›¸ä¼¼åº¦ï¼Œä¿ç•™æ›´é«˜
                    if unique_key in results_dict:
                        if score > (results_dict[unique_key].get('score') or 0):
                            results_dict[unique_key] = {
                                'text': summary,  # åªä¿å­˜æ‘˜è¦
                                'full_text': text,  # ä¿å­˜å®Œæ•´æ–‡æœ¬ä½œä¸ºbackup
                                'metadata': metadata,
                                'score': score
                            }
                    else:
                        results_dict[unique_key] = {
                            'text': summary,  # åªä¿å­˜æ‘˜è¦
                            'full_text': text,  # ä¿å­˜å®Œæ•´æ–‡æœ¬ä½œä¸ºbackup
                            'metadata': metadata,
                            'score': score
                        }
            
            # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æŒ‰ç›¸ä¼¼åº¦æ’åº
            results = list(results_dict.values())
            results.sort(key=lambda x: x.get('score', 0), reverse=True)
            
            return {
                'response': str(response),
                'results': results,
                'analysis': analysis
            }
        except Exception as e:
            raise RuntimeError(f"æœç´¢å¤±è´¥: {str(e)}")

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/home')
def home():
    return render_template('home.html')

@app.route('/index2')
def index2():
    return render_template('index2.html')

@app.route('/.well-known/<path:path>')
def well_known(path):
    return '', 204

@app.route('/favicon.ico')
def favicon():
    return '', 204

@app.route('/api/transcribe', methods=['POST'])
def transcribe():
    if 'file' not in request.files:
        return jsonify({'success': False, 'error': 'æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶'}), 400
    
    file = request.files['file']
    if file.filename == '' or not allowed_file(file.filename):
        return jsonify({'success': False, 'error': 'æ— æ•ˆçš„æ–‡ä»¶'}), 400
    
    try:
        filename = secure_filename(file.filename)
        filepath = UPLOAD_FOLDER / filename
        file.save(str(filepath))
        
        output_path = TRANSCRIPTS_FOLDER / f"{Path(filename).stem}.txt"
        
        if output_path.exists():
            return jsonify({
                'success': True,
                'transcript': output_path.read_text(encoding='utf-8', errors='ignore'),
                'filename': filename,
                'output_path': str(output_path)
            })
        
        transcript = transcribe_with_whisper(str(filepath))
        
        output_path.write_text(transcript, encoding='utf-8')
        
        return jsonify({
            'success': True,
            'transcript': transcript,
            'filename': filename,
            'output_path': str(output_path)
        })
    
    except Exception as e:
        error_trace = traceback.format_exc()
        print(f"è½¬å½•å¼‚å¸¸: {error_trace}")
        return jsonify({'success': False, 'error': str(e), 'detail': error_trace}), 500

@app.route('/api/clean-transcript', methods=['POST'])
def clean_transcript_route():
    data = request.json
    transcript = data.get('transcript', '')
    filename = data.get('filename', 'unknown')
    
    if not transcript:
        return jsonify({'success': False, 'error': 'è½¬å½•æ–‡æœ¬ä¸ºç©º'}), 400
    
    try:
        cleaned = clean_text(transcript)
        output_path = TRANSCRIPTS_CLEAN_FOLDER / f"{Path(filename).stem}.txt"
        output_path.write_text(cleaned, encoding='utf-8')
        
        return jsonify({
            'success': True,
            'cleaned_transcript': cleaned,
            'output_path': str(output_path)
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/extract-keywords', methods=['POST'])
def extract_keywords():
    data = request.json
    transcript = data.get('transcript', '')
    description = data.get('description', '')
    use_openai = data.get('use_openai', False)
    use_gemini = data.get('use_gemini', False)
    api_key = data.get('api_key', '')
    
    if not transcript and not description:
        return jsonify({'success': False, 'error': 'å†…å®¹ä¸ºç©º'}), 400
    
    try:
        if (use_gemini or use_openai) and api_key:
            prompt = generate_keyword_extraction_prompt(description, transcript)
            keywords = []
            method = ""
            
            if use_gemini and GEMINI_AVAILABLE:
                client = create_gemini_client(api_key)
                response = client.models.generate_content(
                    model=GEMINI_MODEL,
                    contents=[types.Content(parts=[types.Part(text=prompt)])],
                    config=types.GenerateContentConfig(response_mime_type="application/json", temperature=0.3)
                )
                result = json.loads(response.text.strip())
                keywords = result.get('keywords', [])
                method = 'gemini'
                
            elif use_openai:
                client = create_openai_client(api_key)
                resp = client.chat.completions.create(
                    model=OPENAI_MODEL,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¿¡æ¯æŠ½å–åŠ©æ‰‹ï¼Œåªè¾“å‡ºJSONæ ¼å¼ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    response_format={"type": "json_object"},
                    temperature=0.3, max_completion_tokens=400
                )
                result = json.loads(resp.choices[0].message.content.strip())
                keywords = result.get('keywords', [])
                method = 'openai'
            
            return jsonify({
                'success': True,
                'keywords': keywords,
                'keywords_str': 'ï¼Œ'.join(keywords),
                'method': method
            })
            
        else:
            combined_text = (description + "\n" + transcript).strip()
            keywords_dict = extract_keywords_by_category(combined_text)
            return jsonify({
                'success': True,
                'keywords': keywords_dict['all'],
                'keywords_methodology': keywords_dict['methodology'],
                'keywords_str': 'ï¼Œ'.join(keywords_dict['all']),
                'method': 'jieba'
            })
    
    except Exception as e:
        return jsonify({'success': False, 'error': f'å…³é”®è¯æå–å¤±è´¥: {str(e)}'}), 500

@app.route('/api/generate-abstract', methods=['POST'])
def generate_abstract():
    data = request.json
    original_abstract = data.get('original_abstract', '')
    keywords = data.get('keywords', '')
    title = data.get('title', '')
    speaker_name = data.get('speaker_name', '')
    transcript = data.get('transcript', '')
    api_key = data.get('api_key', '')
    abstract_length = data.get('abstract_length', 400)
    main_source = data.get('main_source', 'balanced')
    
    if not api_key:
        return jsonify({'success': False, 'error': 'éœ€è¦ OpenAI API Key'}), 400
    
    try:
        client = create_openai_client(api_key)
        
        input_parts = []
        if data.get('use_original_abstract') and original_abstract:
            input_parts.append(f"- åŸæ‘˜è¦ descï¼š{original_abstract[:8000]}")
        if data.get('use_keywords') and keywords:
            input_parts.append(f"- å…³é”®è¯ï¼š{keywords}")
        if data.get('use_transcript') and transcript:
            input_parts.append(f"- è½¬å†™ transcriptï¼š{transcript[:3000]}")
            
        if not input_parts:
            return jsonify({'success': False, 'error': 'æœªé€‰æ‹©ä»»ä½•è¾“å…¥æº'}), 400
            
        priority_note = {
            'transcript': "ã€é‡è¦ã€‘ä»¥è½¬å†™æ–‡æœ¬ä¸ºä¸»è¦æ¥æºã€‚",
            'abstract': "ã€é‡è¦ã€‘ä»¥åŸæ‘˜è¦ä¸ºä¸»è¦æ¥æºã€‚",
            'balanced': "ã€é‡è¦ã€‘å¹³è¡¡ä½¿ç”¨æ‰€æœ‰æ¥æºã€‚"
        }.get(main_source, "")
        
        min_len, max_len = int(abstract_length * 0.9), int(abstract_length * 1.1)
        
        prompt = f"""ä½ æ˜¯å­¦æœ¯ç¼–è¾‘ã€‚è¯·ç”Ÿæˆ {min_len}â€“{max_len} å­—çš„ä¸­æ–‡å­¦æœ¯æ‘˜è¦ã€‚
è¾“å…¥ï¼šæ ‡é¢˜ï¼š{title}, æŠ¥å‘Šäººï¼š{speaker_name}
{chr(10).join(input_parts)}
{priority_note}
è¦æ±‚ï¼šæœ¯è¯­è§„èŒƒï¼Œä¸ç¼–é€ äº‹å®ã€‚
è¾“å‡ºJSON: {{"abstract": "æ­£æ–‡", "self_check": {{...}}}}"""

        resp = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": "åªè¾“å‡ºJSONã€‚"},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.3,
            max_completion_tokens=max(400, int(abstract_length * 1.5))
        )
        
        result = json.loads(resp.choices[0].message.content.strip())
        return jsonify({'success': True, 'abstract': result.get('abstract', '')})
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/download-transcript', methods=['GET'])
def download_transcript():
    filename = request.args.get('filename', '')
    filepath = TRANSCRIPTS_FOLDER / filename
    if not filename or not filepath.exists():
        return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
    return send_file(str(filepath), as_attachment=True, download_name=filename)

@app.route('/api/process-video-url', methods=['POST'])
def process_video_url_api():
    try:
        data = request.json
        video_url = data.get('video_url', '').strip()
        ai_model = data.get('ai_model', 'gemini')
        api_key = data.get('api_key', '')
        few_shot_text = data.get('few_shot_text', FEW_SHOT_EXAMPLE_TEXT)
        
        if not video_url or not api_key:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
            
        if not GEMINI_AVAILABLE:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘ä¾èµ–åº“ yt-dlp/trafilatura'}), 500
            
        from gemini_try import download_video, compress_video
        
        local_video_path = None
        compressed_file_path = None
        
        try:
            local_video_path, web_text = download_video(video_url, "temp_lecture.mp4")
            if not local_video_path:
                return jsonify({'success': False, 'error': 'è§†é¢‘ä¸‹è½½å¤±è´¥'}), 500

            if ai_model == 'gemini':
                if not GEMINI_AVAILABLE:
                    return jsonify({'success': False, 'error': 'Geminiæ¨¡å—ä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install google-genai'}), 500
                
                # ç¡®ä¿typesæ¨¡å—å·²å¯¼å…¥
                from google.genai import types
                
                client = create_gemini_client(api_key)
                
                file_size = os.path.getsize(local_video_path)
                upload_path = local_video_path
                if file_size > 2.0 * 1024 * 1024 * 1024: # > 2GB
                    compressed_file_path = compress_video(local_video_path)
                    if compressed_file_path: upload_path = compressed_file_path

                video_file = client.files.upload(file=upload_path)
                
                try:
                    start_time = time.time()
                    while video_file.state.name == "PROCESSING":
                        if time.time() - start_time > 600:
                            raise TimeoutError("Gemini è§†é¢‘å¤„ç†è¶…æ—¶")
                        time.sleep(5)
                        video_file = client.files.get(name=video_file.name)
                    
                    if video_file.state.name == "FAILED":
                        raise Exception(f"Gemini å¤„ç†å¤±è´¥: {video_file.error.message}")
                    
                    prompt = f"""åˆ†æè¿™ä¸ªå­¦æœ¯æŠ¥å‘Šè§†é¢‘ï¼Œæå–ä»¥ä¸‹ä¿¡æ¯ï¼š

1. **è½¬å½•æ–‡æœ¬(transcript)**ï¼šè¯·å°†è§†é¢‘ä¸­çš„è¯­éŸ³å†…å®¹å®Œæ•´è½¬å½•ä¸ºæ–‡æœ¬ï¼ˆåŒ…æ‹¬æ¼”è®²è€…çš„æ‰€æœ‰è®ºè¿°ï¼‰
   - è½¬å½•æ–‡æœ¬è¦å®Œæ•´ã€å‡†ç¡®ï¼Œä¿ç•™ä¸“ä¸šæœ¯è¯­
   - èƒ½è¯†åˆ«å¹¶æ ¡æ­£ä¸“ä¸šæœ¯è¯­é”™è¯¯ï¼Œä½¿ç”¨è§„èŒƒçš„å­¦æœ¯æœ¯è¯­

2. **å…³é”®è¯(keywords)**ï¼šä»è½¬å½•æ–‡æœ¬ä¸­æå–10-15ä¸ªæ ¸å¿ƒå­¦æœ¯æœ¯è¯­å…³é”®è¯
   
   æå–è¦æ±‚ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š
   - ã€å­¦æœ¯æœ¯è¯­ä¼˜å…ˆã€‘ä¼˜å…ˆæå–ä¸“ä¸šæœ¯è¯­ã€æŠ€æœ¯åè¯ã€å­¦ç§‘æ¦‚å¿µè¯
     * é•¿è¯ï¼ˆâ‰¥3å­—ï¼‰ä¼˜å…ˆï¼šå¦‚"ç¥ç»ç½‘ç»œ"ã€"ç”µåŒ–å­¦"ã€"é’™é’›çŸ¿"ã€"ç¦»å­ä¼ è¾“"
     * ä¸“ä¸šåè¯ä¼˜å…ˆï¼šå¦‚"æ™¶ä½“ç®¡"ã€"å™¨ä»¶"ã€"ç”µæ± "ã€"ç®—æ³•"
     * é¿å…é€šç”¨è¯ï¼šå¦‚"ç ”ç©¶"ã€"æ–¹æ³•"ã€"ç³»ç»Ÿ"ã€"åŠŸèƒ½"ã€"æ€§èƒ½"ã€"å®ç°"ã€"åº”ç”¨"ã€"å‘å±•"ã€"é—®é¢˜"ã€"æŠ€æœ¯"ã€"ç•Œé¢"ã€"ç­–ç•¥"ã€"æ–¹æ¡ˆ"
   
   - ã€è¿‡æ»¤è§„åˆ™ã€‘å¿…é¡»æ’é™¤ï¼š
     * æ³›è¯ï¼šç ”ç©¶/æ–¹æ³•/æŠ€æœ¯/ç³»ç»Ÿ/åŠŸèƒ½/æ€§èƒ½/å®ç°/åº”ç”¨/å‘å±•/é—®é¢˜/ç•Œé¢/ç­–ç•¥/æ–¹æ¡ˆ/è¿‡ç¨‹/ç»“æœ/æ•°æ®/ä¿¡æ¯/å†…å®¹/æ–¹é¢/é¢†åŸŸ/æ–¹å‘/å·¥ä½œ/é¡¹ç›®
     * å£è¯­åŒ–è¯æ±‡ï¼šå¦‚"åŸºæœ¬ä¸Š"ã€"æœ‰æ²¡æœ‰"ã€"ç›¸å½“äº"ã€"èˆ’æœ"ã€"è‚¯å®š"ã€"è¯å‡ºæ¥"ç­‰
     * åŠ¨è¯æ€§é€šç”¨è¯ï¼šå¦‚"èƒ½å¤Ÿ"ã€"æ”¶è·"ã€"æ¥æ°´"ç­‰
     * æœºæ„åã€äººåã€åœ°å
     * ä½ä¿¡æ¯é‡è¯
   
   - ã€è¯æ€§åå¥½ã€‘ä¼˜å…ˆåè¯æ€§è¯æ±‡ï¼ˆä¸“ä¸šæœ¯è¯­é€šå¸¸æ˜¯åè¯ï¼‰ï¼Œå‡å°‘åŠ¨è¯/å½¢å®¹è¯/å‰¯è¯
   
   - ã€è´¨é‡è¦æ±‚ã€‘ç¡®ä¿æ¯ä¸ªå…³é”®è¯éƒ½æ˜¯è¯¥é¢†åŸŸçš„æ ¸å¿ƒå­¦æœ¯æœ¯è¯­ï¼Œå…·æœ‰æ˜ç¡®çš„å­¦æœ¯å«ä¹‰å’ŒæŠ€æœ¯ä»·å€¼

ç½‘é¡µç®€ä»‹å‚è€ƒï¼š{web_text if web_text else 'ï¼ˆæ— ç½‘é¡µç®€ä»‹ï¼‰'}

è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š
{{
    "transcript": "å®Œæ•´çš„è½¬å½•æ–‡æœ¬å†…å®¹...",
    "keywords": ["å…³é”®è¯1", "å…³é”®è¯2", "å…³é”®è¯3", ...]
}}"""
        
                    response = client.models.generate_content(
                        model=GEMINI_MODEL,
                        contents=[
                            types.Content(parts=[
                                types.Part(file_data=types.FileData(file_uri=video_file.uri, mime_type=video_file.mime_type)),
                                types.Part(text=prompt)
                            ])
                        ],
                        config=types.GenerateContentConfig(response_mime_type="application/json", temperature=0.2)
                    )
                    
                    # å®‰å…¨è§£æJSONå“åº”
                    try:
                        res_data = json.loads(response.text)
                    except json.JSONDecodeError as e:
                        print(f"JSONè§£æå¤±è´¥: {e}")
                        print(f"å“åº”å†…å®¹: {response.text[:500]}")
                        raise Exception(f"Geminiè¿”å›çš„JSONæ ¼å¼æ— æ•ˆ: {str(e)}")
                    
                    transcript = res_data.get('transcript', '')
                    keywords = res_data.get('keywords', [])
                    
                    expert_opinion = None
                    if few_shot_text and few_shot_text.strip():
                        try:
                            op_prompt = generate_expert_opinion_prompt(web_text, "", few_shot_text, has_video=True)
                            op_res = client.models.generate_content(
                                model=GEMINI_MODEL,
                                contents=[types.Content(parts=[
                                    types.Part(file_data=types.FileData(file_uri=video_file.uri, mime_type=video_file.mime_type)),
                                    types.Part(text=op_prompt)
                                ])],
                                config=types.GenerateContentConfig(response_mime_type="application/json", temperature=0.2)
                            )
                            try:
                                expert_opinion = json.loads(op_res.text)
                            except json.JSONDecodeError as e:
                                print(f"ä¸“å®¶è§‚ç‚¹JSONè§£æå¤±è´¥: {e}")
                                print(f"å“åº”å†…å®¹: {op_res.text[:500]}")
                                expert_opinion = {'raw_text': op_res.text, 'error': 'JSONè§£æå¤±è´¥'}
                        except Exception as e:
                            print(f"ç”Ÿæˆä¸“å®¶è§‚ç‚¹å¤±è´¥ï¼ˆä¸å½±å“è½¬å½•å’Œå…³é”®è¯æå–ï¼‰: {e}")
                            traceback.print_exc()

                    return jsonify({
                        'success': True,
                        'transcript': transcript,
                        'keywords': keywords,
                        'keywords_str': 'ï¼Œ'.join(keywords),
                        'web_text': web_text,
                        'expert_opinion': expert_opinion,
                        'model': 'gemini'
                    })

                finally:
                    if 'video_file' in locals() and hasattr(video_file, 'name'):
                        try: client.files.delete(name=video_file.name)
                        except: pass

            elif ai_model in ['chatgpt', 'deepseek', 'qwen']:
                transcript = transcribe_with_whisper(local_video_path)
                
                # ä½¿ç”¨AIæ¨¡å‹æå–å…³é”®è¯ï¼ˆè€Œä¸æ˜¯åªç”¨Jiebaï¼‰
                keywords = []
                keywords_str = ''
                try:
                    keyword_prompt = generate_keyword_extraction_prompt(web_text, transcript)
                    
                    if ai_model == 'chatgpt':
                        client = create_openai_client(api_key)
                        resp = client.chat.completions.create(
                            model=OPENAI_MODEL,
                            messages=[
                                {"role": "system", "content": "ä½ æ˜¯ä¿¡æ¯æŠ½å–åŠ©æ‰‹ï¼Œåªè¾“å‡ºJSONæ ¼å¼ã€‚"},
                                {"role": "user", "content": keyword_prompt}
                            ],
                            response_format={"type": "json_object"},
                            temperature=0.3,
                            max_completion_tokens=400
                        )
                        result = json.loads(resp.choices[0].message.content.strip())
                        keywords = result.get('keywords', [])
                        keywords_str = 'ï¼Œ'.join(keywords)
                    
                    elif ai_model == 'deepseek':
                        if not DEEPSEEK_AVAILABLE:
                            print("DeepSeek SDK æœªå®‰è£…ï¼Œä½¿ç”¨Jiebaæå–å…³é”®è¯")
                            keywords_dict = extract_keywords_by_category(transcript)
                            keywords = keywords_dict['all']
                            keywords_str = 'ï¼Œ'.join(keywords)
                        else:
                            client = DeepSeekClient(api_key=api_key, base_url="https://api.deepseek.com")
                            resp = client.chat.completions.create(
                                model=DEEPSEEK_MODEL,
                                messages=[
                                    {"role": "system", "content": "ä½ æ˜¯ä¿¡æ¯æŠ½å–åŠ©æ‰‹ï¼Œåªè¾“å‡ºJSONæ ¼å¼ã€‚"},
                                    {"role": "user", "content": keyword_prompt}
                                ],
                                response_format={"type": "json_object"},
                                temperature=0.3
                            )
                            result = json.loads(resp.choices[0].message.content.strip())
                            keywords = result.get('keywords', [])
                            keywords_str = 'ï¼Œ'.join(keywords)
                    
                    elif ai_model == 'qwen':
                        client = create_qwen_client(api_key)
                        resp = client.chat.completions.create(
                            model=QWEN_MODEL,
                            messages=[
                                {"role": "system", "content": "ä½ æ˜¯ä¿¡æ¯æŠ½å–åŠ©æ‰‹ï¼Œåªè¾“å‡ºJSONæ ¼å¼ã€‚"},
                                {"role": "user", "content": keyword_prompt}
                            ],
                            response_format={"type": "json_object"},
                            temperature=0.3
                        )
                        result = json.loads(resp.choices[0].message.content.strip())
                        keywords = result.get('keywords', [])
                        keywords_str = 'ï¼Œ'.join(keywords)
                        
                except Exception as e:
                    print(f"AIå…³é”®è¯æå–å¤±è´¥ï¼Œå›é€€åˆ°Jieba: {e}")
                    traceback.print_exc()
                    # å›é€€åˆ°Jieba
                    keywords_dict = extract_keywords_by_category(transcript)
                    keywords = keywords_dict['all']
                    keywords_str = 'ï¼Œ'.join(keywords)
                
                # å¦‚æœæä¾›äº†Few-Shotæ–‡æœ¬ï¼Œåœ¨æ­¥éª¤1å°±ç”Ÿæˆä¸“å®¶è§‚ç‚¹
                expert_opinion = None
                if few_shot_text and few_shot_text.strip():
                    try:
                        prompt = generate_expert_opinion_prompt(web_text, transcript, few_shot_text, has_video=False)
                        
                        if ai_model == 'chatgpt':
                            client = create_openai_client(api_key)
                            resp = client.chat.completions.create(
                                model=OPENAI_MODEL,
                                messages=[{"role": "system", "content": "åªè¾“å‡ºJSONã€‚"}, {"role": "user", "content": prompt}],
                                response_format={"type": "json_object"}, temperature=0.2
                            )
                            expert_opinion = json.loads(resp.choices[0].message.content.strip())
                        
                        elif ai_model == 'deepseek':
                            if not DEEPSEEK_AVAILABLE:
                                print("DeepSeek SDK æœªå®‰è£…ï¼Œè·³è¿‡ä¸“å®¶è§‚ç‚¹ç”Ÿæˆ")
                            else:
                                client = DeepSeekClient(api_key=api_key, base_url="https://api.deepseek.com")
                                resp = client.chat.completions.create(
                                    model=DEEPSEEK_MODEL,
                                    messages=[{"role": "system", "content": "åªè¾“å‡ºJSONã€‚"}, {"role": "user", "content": prompt}],
                                    response_format={"type": "json_object"}, temperature=0.2
                                )
                                expert_opinion = json.loads(resp.choices[0].message.content.strip())
                                print(f"DeepSeekä¸“å®¶è§‚ç‚¹ç”ŸæˆæˆåŠŸ: {len(str(expert_opinion))} å­—ç¬¦")
                        
                        elif ai_model == 'qwen':
                            client = create_qwen_client(api_key)
                            resp = client.chat.completions.create(
                                model=QWEN_MODEL,
                                messages=[{"role": "system", "content": "åªè¾“å‡ºJSONã€‚"}, {"role": "user", "content": prompt}],
                                response_format={"type": "json_object"}, temperature=0.2
                            )
                            expert_opinion = json.loads(resp.choices[0].message.content.strip())
                    except Exception as e:
                        print(f"æ­¥éª¤1ç”Ÿæˆä¸“å®¶è§‚ç‚¹å¤±è´¥ï¼ˆä¸å½±å“è½¬å½•å’Œå…³é”®è¯æå–ï¼‰: {e}")
                        traceback.print_exc()
                
                return jsonify({
                    'success': True,
                    'transcript': transcript,
                    'keywords': keywords,
                    'keywords_str': keywords_str,
                    'web_text': web_text,
                    'expert_opinion': expert_opinion,  # å¦‚æœæœ‰Few-Shotæ–‡æœ¬ï¼Œä¼šç”Ÿæˆä¸“å®¶è§‚ç‚¹
                    'model': ai_model
                })
            
            else:
                return jsonify({'success': False, 'error': f'ä¸æ”¯æŒçš„æ¨¡å‹: {ai_model}'}), 400

        except Exception as e:
            error_trace = traceback.format_exc()
            print(f"å¤„ç†è§†é¢‘é“¾æ¥å¼‚å¸¸: {error_trace}")
            try:
                return jsonify({
                    'success': False,
                    'error': f'å¤„ç†å¤±è´¥: {str(e)}',
                    'detail': error_trace[-1000:] if len(error_trace) > 1000 else error_trace
                }), 500
            except Exception as json_error:
                # å¦‚æœè¿JSONéƒ½æ— æ³•è¿”å›ï¼Œè‡³å°‘è¿”å›ä¸€ä¸ªç®€å•çš„å“åº”
                return json.dumps({
                    'success': False,
                    'error': f'å¤„ç†å¤±è´¥: {str(e)}'
                }), 500, {'Content-Type': 'application/json'}
        
        finally:
            cleanup_files(local_video_path, compressed_file_path)
            
    except Exception as e:
        error_trace = traceback.format_exc()
        print(f"è¯·æ±‚è§£æå¼‚å¸¸: {error_trace}")
        try:
            return jsonify({
                'success': False,
                'error': f'è¯·æ±‚è§£æå¤±è´¥: {str(e)}',
                'detail': error_trace[-500:] if len(error_trace) > 500 else error_trace
            }), 400
        except Exception as json_error:
            return json.dumps({
                'success': False,
                'error': f'è¯·æ±‚è§£æå¤±è´¥: {str(e)}'
            }), 400, {'Content-Type': 'application/json'}

@app.route('/api/generate-expert-opinion', methods=['POST'])
def generate_expert_opinion_route():
    data = request.json
    transcript = data.get('transcript', '')
    web_text = data.get('web_text', '')
    few_shot_text = data.get('few_shot_text', FEW_SHOT_EXAMPLE_TEXT)
    ai_model = data.get('ai_model', 'chatgpt')
    api_key = data.get('api_key', '')
    
    if not (transcript or web_text) or not api_key:
        return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
    try:
        prompt = generate_expert_opinion_prompt(web_text, transcript, few_shot_text, has_video=False)
        result = {}
        
        if ai_model == 'chatgpt':
            client = create_openai_client(api_key)
            resp = client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[{"role": "system", "content": "åªè¾“å‡ºJSONã€‚"}, {"role": "user", "content": prompt}],
                response_format={"type": "json_object"}, temperature=0.2
            )
            result = json.loads(resp.choices[0].message.content.strip())
            
        elif ai_model == 'deepseek':
            if not DEEPSEEK_AVAILABLE: raise Exception("DeepSeek SDK æœªå®‰è£…")
            client = DeepSeekClient(api_key=api_key, base_url="https://api.deepseek.com")
            resp = client.chat.completions.create(
                model=DEEPSEEK_MODEL,
                messages=[{"role": "system", "content": "åªè¾“å‡ºJSONã€‚"}, {"role": "user", "content": prompt}],
                response_format={"type": "json_object"}, temperature=0.2
            )
            result = json.loads(resp.choices[0].message.content.strip())
            
        elif ai_model == 'qwen':
            client = create_qwen_client(api_key)
            resp = client.chat.completions.create(
                model=QWEN_MODEL,
                messages=[{"role": "system", "content": "åªè¾“å‡ºJSONã€‚"}, {"role": "user", "content": prompt}],
                response_format={"type": "json_object"}, temperature=0.2
            )
            result = json.loads(resp.choices[0].message.content.strip())
            
        elif ai_model == 'gemini':
            client = create_gemini_client(api_key)
            resp = client.models.generate_content(
                model=GEMINI_MODEL,
                contents=[types.Content(parts=[types.Part(text=prompt)])],
                config=types.GenerateContentConfig(response_mime_type="application/json", temperature=0.2)
            )
            result = json.loads(resp.text.strip())
            
        return jsonify({'success': True, 'result': result, 'model': ai_model})
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/rag/search', methods=['POST'])
def rag_search():
    try:
        data = request.json
        query = data.get('query', '').strip()
        categories_filter = data.get('categories', [])
        api_key = data.get('api_key', '')
        
        if not query:
            return jsonify({'success': False, 'error': 'æœç´¢å…³é”®è¯ä¸èƒ½ä¸ºç©º'}), 400
        
        if not api_key:
            return jsonify({'success': False, 'error': 'éœ€è¦Gemini API Keyè¿›è¡ŒæŸ¥è¯¢åˆ†æï¼Œè¯·åœ¨Video Content Analysisé¡µé¢é…ç½®'}), 400
        
        # åˆå§‹åŒ–RAGå¼•æ“ï¼ˆå»¶è¿ŸåŠ è½½ï¼Œä¼ é€’API Keyï¼‰
        try:
            engine = init_rag_engine(api_key=api_key)
        except Exception as e:
            return jsonify({
                'success': False,
                'error': f'RAGå¼•æ“åˆå§‹åŒ–å¤±è´¥: {str(e)}',
                'hint': 'è¯·æ£€æŸ¥æ•°æ®åº“è¿æ¥å’Œé…ç½®'
            }), 500
        
        # æ‰§è¡Œæœç´¢
        try:
            search_result = engine.search(query, api_key, categories_filter)
            
            return jsonify({
                'success': True,
                'results': search_result['results'],
                'analysis': search_result['analysis'],
                'response': search_result['response']
            })
        except Exception as e:
            error_msg = str(e)
            if "password authentication failed" in error_msg.lower():
                return jsonify({
                    'success': False,
                    'error': 'æ•°æ®åº“è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®åº“å¯†ç é…ç½®'
                }), 500
            elif "connection" in error_msg.lower():
                return jsonify({
                    'success': False,
                    'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®åº“æ˜¯å¦è¿è¡Œ'
                }), 500
            else:
                return jsonify({
                    'success': False,
                    'error': f'æœç´¢å¤±è´¥: {error_msg}'
                }), 500
        
    except Exception as e:
        error_trace = traceback.format_exc()
        print(f"RAGæœç´¢å¼‚å¸¸: {error_trace}")
        return jsonify({
            'success': False,
            'error': f'æœç´¢å¤±è´¥: {str(e)}',
            'detail': error_trace[-500:] if len(error_trace) > 500 else error_trace
        }), 500

def crawl_latest_report_urls():
    """
    [çœŸå®çˆ¬è™«] è·å–æœ€æ–°å¹´ä»½çš„æ‰€æœ‰æŠ¥å‘Š URL
    """
    print("ğŸ•·ï¸ å¼€å§‹çˆ¬å– CAS å®˜ç½‘æœ€æ–°æŠ¥å‘Šåˆ—è¡¨...")
    root = "https://academics.casad.cas.cn/xshd/kxyjsqylt/"
    
    all_report_urls = []
    
    try:
        # 1. è·å–æ‰€æœ‰å¹´ä»½
        years = parse_year_links(root)
        if not years:
            print("âš ï¸ æœªæ‰¾åˆ°å¹´ä»½ä¿¡æ¯")
            return []
            
        # 2. ã€ç­–ç•¥ã€‘åªçˆ¬å–æœ€æ–°çš„ 1 ä¸ªå¹´ä»½ (é€šå¸¸æ˜¯åˆ—è¡¨ç¬¬ä¸€ä¸ª)
        # å¦‚æœéœ€è¦çˆ¬æ›´å¤šå¹´ä»½ï¼Œå¯ä»¥ä¿®æ”¹åˆ‡ç‰‡ï¼Œä¾‹å¦‚ years[:2]
        latest_year = years[0] 
        print(f"ğŸ“… æ­£åœ¨å¤„ç†æœ€æ–°å¹´ä»½: {latest_year['year']}")
        
        # 3. è·å–è¯¥å¹´ä»½ä¸‹çš„æ‰€æœ‰è®ºå› (d204c, d203c...)
        forums = parse_year_forums(latest_year["year_url"])
        print(f"ğŸ“š å‘ç° {len(forums)} ä¸ªè®ºå›ç‰ˆå—")
        
        # 4. éå†æ¯ä¸ªè®ºå›ï¼Œè·å–é‡Œé¢çš„æ–‡ç« é“¾æ¥
        for f in forums:
            # print(f"  æ­£åœ¨æŠ“å–ç‰ˆå—: {f['forum_id']}...")
            # ä¸ºäº†é€Ÿåº¦ï¼Œè¿™é‡Œåªçˆ¬å–æ¯ä¸ªç‰ˆå—çš„ç¬¬ 1 é¡µ (index.html)
            # å¦‚æœä½ æƒ³çˆ¬æ¯ä¸ªç‰ˆå—çš„æ‰€æœ‰åˆ†é¡µï¼Œéœ€è¦å¼•å…¥ build_page_urls é€»è¾‘
            forum_urls = parse_page_urls(f["forum_url"])
            all_report_urls.extend(forum_urls)
            # ç¤¼è²Œæ€§å»¶æ—¶ï¼Œé˜²æ­¢è¢«å° IP
            time.sleep(0.1)
            
    except Exception as e:
        print(f"âŒ çˆ¬è™«å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        
    print(f"âœ… çˆ¬å–å®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_report_urls)} ä¸ªæŠ¥å‘Šé“¾æ¥")
    # å»é‡
    return list(set(all_report_urls))


def background_update_task(api_key):
    """åå°æ‰§è¡Œçš„æ›´æ–°ä»»åŠ¡"""
    print(f"[{datetime.now()}] ğŸš€ å¼€å§‹æ‰§è¡ŒçŸ¥è¯†åº“æ›´æ–°ä»»åŠ¡...")
    
    try:
        # 1. åˆå§‹åŒ– RAG å¼•æ“ (è¿æ¥æ•°æ®åº“)
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ç¡®ä¿ä½¿ç”¨äº†æ­£ç¡®çš„ Embedding æ¨¡å‹å¯¹åº”çš„ API Key
        # å¦‚æœæ˜¯å›½å†…ç¯å¢ƒï¼Œå»ºè®®æ¢æˆ HuggingFaceEmbedding (æœ¬åœ°æ¨¡å‹) å°±ä¸éœ€è¦è¿™ä¸ª Key äº†
        engine = init_rag_engine(api_key=api_key)
        
        # 2. è·å–æœ€æ–° URL
        latest_urls = crawl_latest_report_urls()
        
        # 3. æ£€æŸ¥æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„ URL (é˜²æ­¢é‡å¤)
        # è¿™é‡Œä½¿ç”¨ç®€å•çš„ SQL æŸ¥è¯¢æˆ–è€…é€šè¿‡ LlamaIndex çš„ docstore æ£€æŸ¥
        # ä¸ºç®€åŒ–æ¼”ç¤ºï¼Œè¿™é‡Œå‡è®¾æ‰€æœ‰çˆ¬åˆ°çš„éƒ½æ˜¯æ–°çš„
        new_urls = latest_urls 
        
        if not new_urls:
            print("æ²¡æœ‰å‘ç°æ–°å†…å®¹ã€‚")
            return

        print(f"å‘ç° {len(new_urls)} ä¸ªæ–°æŠ¥å‘Šï¼Œå¼€å§‹å¤„ç†...")
        
        # 4. å¤„ç†æ–°å†…å®¹ (ä¸‹è½½ -> è½¬å½• -> å‘é‡åŒ– -> å…¥åº“)
        new_documents = []
        
        # âš ï¸ è¿™é‡Œéœ€è¦ä½ å¤ç”¨ build_index.py é‡Œçš„é€»è¾‘æˆ–è€… download_video é€»è¾‘
        # from gemini_try import download_video
        # for url in new_urls:
        #     local_video, web_text = download_video(url, "temp.mp4")
        #     transcript = transcribe_with_whisper(local_video)
        #     doc = Document(text=transcript, metadata={"original_url": url, "summary": web_text})
        #     new_documents.append(doc)
        
        # 5. æ’å…¥æ•°æ®åº“
        if new_documents:
            engine.index.insert_nodes(new_documents)
            print(f"âœ… æˆåŠŸæ›´æ–° {len(new_documents)} æ¡æ•°æ®åˆ°çŸ¥è¯†åº“ï¼")
        else:
            print("âš ï¸ æœªç”Ÿæˆæœ‰æ•ˆæ–‡æ¡£ï¼Œæ›´æ–°è·³è¿‡ã€‚")
            
    except Exception as e:
        print(f"âŒ æ›´æ–°ä»»åŠ¡å¤±è´¥: {str(e)}")
        traceback.print_exc()

@app.route('/admin')
def admin_page():
    """åå°ç®¡ç†é¡µé¢"""
    return render_template('admin.html')

@app.route('/api/admin/update-db', methods=['POST'])
def trigger_update_db():
    """è§¦å‘æ›´æ–°çš„æ¥å£"""
    data = request.json
    password = data.get('password')
    api_key = data.get('api_key')
    
    if password != ADMIN_PASSWORD:
        return jsonify({'success': False, 'error': 'ç®¡ç†å‘˜å¯†ç é”™è¯¯'}), 401
    
    if not api_key:
        return jsonify({'success': False, 'error': 'éœ€è¦æä¾› API Key ç”¨äºç”Ÿæˆå‘é‡'}), 400

    # å¯åŠ¨åå°çº¿ç¨‹ï¼Œé¿å…å¡æ­»ç½‘é¡µ
    thread = threading.Thread(target=background_update_task, args=(api_key,))
    thread.start()
    
    return jsonify({
        'success': True, 
        'message': 'æ›´æ–°ä»»åŠ¡å·²åœ¨åå°å¯åŠ¨ï¼Œè¯·å…³æ³¨æœåŠ¡å™¨æ—¥å¿—æˆ–ç¨ååˆ·æ–°æ•°æ®åº“ã€‚'
    })


if __name__ == '__main__':
    port = 5001
    if len(sys.argv) > 1:
        try: port = int(sys.argv[1])
        except: pass
    
    print(f"\nå¯åŠ¨ Flask åº”ç”¨: http://localhost:{port}\n")
    from werkzeug.serving import WSGIRequestHandler
    WSGIRequestHandler.timeout = 1800
    app.run(debug=True, host='0.0.0.0', port=port, threaded=True)